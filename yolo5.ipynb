{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image  # Import Image for handling image files\n",
    "\n",
    "# Step 3: Dataset Preparation\n",
    "data_path = 'D:\\\\GBC SEM2\\\\Deep Learning\\\\Project\\\\Train'\n",
    "\n",
    "# Apply additional preprocessing or augmentations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Custom Dataset class\n",
    "class TrafficSignDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.images = list(Path(data_path).rglob('*.png'))  # Adjusted to .png format\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        # Adjust to the .png format and label paths if necessary\n",
    "        label_path = str(img_path).replace('.png', '.txt')  # Assuming labels are stored with .txt extension\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = f.read().splitlines()\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "# Adjusted paths to match the actual folder names\n",
    "train_dataset = TrafficSignDataset(os.path.join(data_path, 'Train'), transform=transform)\n",
    "val_dataset = TrafficSignDataset(os.path.join(data_path, 'Test'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Step 4: Load YOLOv5 Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Step 5: Train the Model with Augmented Data and Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "img_size = 640\n",
    "learning_rate = 0.01\n",
    "momentum = 0.937\n",
    "\n",
    "# YOLOv5 Training Loop\n",
    "model.train(data=data_path, epochs=epochs, batch_size=batch_size, imgsz=img_size, lr0=learning_rate, momentum=momentum)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "results = model(val=True)\n",
    "print(results)\n",
    "\n",
    "# Visualization of training progress\n",
    "plt.plot(results['metrics']['precision'], label='Precision')\n",
    "plt.plot(results['metrics']['recall'], label='Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Save the Trained Model\n",
    "model_path = 'yolov5_traffic_sign.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')\n",
    "\n",
    "# Step 8: Inference on New Data\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "img = 'path/to/test_image.png'  # Adjusted to .png format\n",
    "results = model(img)\n",
    "results.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 1: Load Images and Labels\n",
    "data = []\n",
    "labels = []\n",
    "classes = 43  # Change this to 42 if your dataset has 42 classes\n",
    "cur_path = os.getcwd()\n",
    "\n",
    "# Retrieving the images and their labels\n",
    "for i in range(classes):\n",
    "    path = os.path.join(cur_path, 'train', str(i))\n",
    "    images = os.listdir(path)\n",
    "\n",
    "    for a in images:\n",
    "        try:\n",
    "            image = Image.open(os.path.join(path, a)).convert(\"RGB\")\n",
    "            image = image.resize((30, 30))  # Resize to match the CNN preprocessing\n",
    "            image = np.array(image)\n",
    "            data.append(image)\n",
    "            labels.append(i)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {a}: {e}\")\n",
    "\n",
    "# Converting lists into numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "# Splitting training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train/Test shapes: {X_train.shape}, {X_test.shape}, {y_train.shape}, {y_test.shape}\")\n",
    "\n",
    "# Converting the labels into one-hot encoding (this step might be skipped for YOLOv5)\n",
    "y_train = to_categorical(y_train, classes)\n",
    "y_test = to_categorical(y_test, classes)\n",
    "\n",
    "# Step 2: Create a Custom Dataset Class\n",
    "class TrafficSignDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.fromarray(self.images[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Step 3: Define Transforms and Load Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = TrafficSignDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = TrafficSignDataset(X_test, y_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv5 Model\n",
    "from models.yolo import Model\n",
    "\n",
    "# Load model weights\n",
    "model = Model(cfg='D:\\GBC SEM2\\Deep Learning\\Project\\yolov5\\models\\yolov5s.yaml')\n",
    "\n",
    "model.load_state_dict(torch.load('D:\\GBC SEM2\\Deep Learning\\Project\\yolov5s.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'requests>=2.32.0'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/2 failed: Command 'pip install --no-cache-dir \"gitpython>=3.1.30\" \"requests>=2.32.0\" ' returned non-zero exit status 1.\n",
      "Retry 2/2 failed: Command 'pip install --no-cache-dir \"gitpython>=3.1.30\" \"requests>=2.32.0\" ' returned non-zero exit status 1.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  Command 'pip install --no-cache-dir \"gitpython>=3.1.30\" \"requests>=2.32.0\" ' returned non-zero exit status 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2024-8-14 Python-3.10.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s_v6 summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.train() got an unexpected keyword argument 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m momentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.937\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# YOLOv5 Training Loop\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Step 6: Evaluate the Model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m results \u001b[38;5;241m=\u001b[39m model(val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Module.train() got an unexpected keyword argument 'data'"
     ]
    }
   ],
   "source": [
    "# Step 5: Train the Model with Augmented Data and Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "img_size = 640\n",
    "learning_rate = 0.01\n",
    "momentum = 0.937\n",
    "\n",
    "# YOLOv5 Training Loop\n",
    "model.train(data=data, epochs=epochs, batch_size=batch_size, imgsz=img_size, lr0=learning_rate, momentum=momentum)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "results = model(val=True)\n",
    "print(results)\n",
    "\n",
    "# Visualization of training progress\n",
    "plt.plot(results['metrics']['precision'], label='Precision')\n",
    "plt.plot(results['metrics']['recall'], label='Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Save the Trained Model\n",
    "model_path = 'yolov5_traffic_sign.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')\n",
    "\n",
    "# Step 8: Inference on New Data\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "img = 'path/to/test_image.png'  # Adjusted to .png format\n",
    "results = model(img)\n",
    "results.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
